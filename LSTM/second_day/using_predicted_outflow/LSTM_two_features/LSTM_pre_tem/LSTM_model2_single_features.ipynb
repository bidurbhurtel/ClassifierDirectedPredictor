{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from keras import backend as K \n",
    "from keras.engine.training import Model\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Sequential, load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Dropout, Activation, Flatten\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hours</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Outflow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/1/1980</td>\n",
       "      <td>1980</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.992</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/1/1980</td>\n",
       "      <td>1980</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.790</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/1/1980</td>\n",
       "      <td>1980</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.588</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/1/1980</td>\n",
       "      <td>1980</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.404</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/1/1980</td>\n",
       "      <td>1980</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.172</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Dates  Year  Month  Day    Hours  Precipitation  Temperature  Outflow\n",
       "0  10/1/1980  1980     10    1  0:00:00            0.0      102.992      5.0\n",
       "1  10/1/1980  1980     10    1  1:00:00            0.0       97.790      5.0\n",
       "2  10/1/1980  1980     10    1  2:00:00            0.0       92.588      5.0\n",
       "3  10/1/1980  1980     10    1  3:00:00            0.0       87.404      5.0\n",
       "4  10/1/1980  1980     10    1  4:00:00            0.0       85.172      5.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "df = pd.read_csv('../Sub0-RAW.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define train and label column\n",
    "train_cols = [\"Outflow\"]\n",
    "label_cols = [\"Outflow(t+1)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the columns\n",
    "new_df = df.filter(train_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min Max scalr normalizing\n",
    "xtrain_min_max_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "ytrain_min_max_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "xtest_min_max_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "ytest_min_max_scaler = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to shift the time_series data for getting labels\n",
    "def lag_seq(df, n_seq):\n",
    "    for i in range(n_seq):\n",
    "        df['Outflow(t+%d)' %(i+1)] = new_df['Outflow'].shift(-(i+1))\n",
    "    return df\n",
    "\n",
    "# building timeseries data with given timesteps\n",
    "def timeseries(X, Y, Y_actual, time_steps, out_steps):\n",
    "    input_size_0 = X.shape[0] - time_steps\n",
    "    input_size_1 = X.shape[1]\n",
    "    X_values = np.zeros((input_size_0, time_steps, input_size_1))\n",
    "    Y_values = np.zeros((input_size_0,))\n",
    "    Y_values_actual = np.zeros((input_size_0,))\n",
    "    \n",
    "    for i in tqdm_notebook(range(input_size_0)):\n",
    "        X_values[i] = X[i:time_steps+i]\n",
    "        Y_values[i] = Y[time_steps+i-1, 0]\n",
    "        Y_values_actual[i] = Y_actual[time_steps+i-1, 0]\n",
    "        \n",
    "    print(\"length of time-series i/o\",X_values.shape,Y_values.shape)\n",
    "    return X_values, Y_values, Y_values_actual\n",
    "\n",
    "#getting data ready for training the model\n",
    "def data_processing(lag_df):\n",
    "    #Splitting training and test data\n",
    "    df_train, df_test = train_test_split(lag_df, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "    x_train = df_train.loc[:,train_cols].values\n",
    "    y_train = df_train.loc[:,label_cols].values\n",
    "    x_test = df_test.loc[:,train_cols].values\n",
    "    y_test = df_test.loc[:,label_cols].values    \n",
    "   \n",
    "    #Normalizing training data\n",
    "    x_train_nor = xtrain_min_max_scaler.fit_transform(x_train)\n",
    "    y_train_nor = ytrain_min_max_scaler.fit_transform(y_train)\n",
    "\n",
    "    # Normalizing test data\n",
    "    x_test_nor = xtest_min_max_scaler.fit_transform(x_test)\n",
    "    y_test_nor = ytest_min_max_scaler.fit_transform(y_test)\n",
    "    \n",
    "    # Saving actual train and test y_label\n",
    "    y_train_actual = y_train\n",
    "    y_test_actual = y_test\n",
    "    \n",
    "    return x_train_nor, y_train_nor, y_train_actual, x_test_nor, y_test_nor, y_test_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling function to create lag dataframe\n",
    "lag_df = lag_seq(new_df, 1)\n",
    "lag_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bidur/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85c6d808b11476e8fcd5469b7c66a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=148416.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (148416, 18, 1) (148416,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c0b3b43c0240d99a45e0261fe4af60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=37091.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (37091, 18, 1) (37091,)\n"
     ]
    }
   ],
   "source": [
    "#Building timeseries\n",
    "x_train_nor, y_train_nor, y_train_actual, x_test_nor, y_test_nor, y_test_actual = data_processing(lag_df)\n",
    "X_Train, Y_Train, Y_train_actual = timeseries(x_train_nor, y_train_nor, y_train_actual, time_steps=18, out_steps=1)\n",
    "X_Test, Y_Test, Y_test_actual = timeseries(x_test_nor, y_test_nor, y_test_actual, time_steps=18, out_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build Model\n",
    "def make_model(X_Train):\n",
    "    input_layer = Input(shape=(X_Train.shape[1], X_Train.shape[2]))\n",
    "\n",
    "    lstm1 = LSTM(units=16, return_sequences=True)(input_layer)\n",
    "    dropout1 = Dropout(0.2)(lstm1)\n",
    "    lstm2 = LSTM(units=32, return_sequences=True)(dropout1)\n",
    "    dropout2 = Dropout(0.2)(lstm2)\n",
    "    lstm3 = LSTM(units=64)(dropout2)\n",
    "    dropout3 = Dropout(0.2)(lstm3)\n",
    "\n",
    "    dense1 = Dense(128, activation='relu')(dropout3)\n",
    "    dense2 = Dense(64, activation='relu')(dense1)\n",
    "    output_layer = Dense(1, activation='linear')(dense2)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 95s 20ms/step - loss: 1.2994e-04 - val_loss: 7.1362e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 93s 20ms/step - loss: 5.5565e-05 - val_loss: 4.1709e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 93s 20ms/step - loss: 3.9301e-05 - val_loss: 1.6506e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 93s 20ms/step - loss: 3.7763e-05 - val_loss: 3.2022e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 93s 20ms/step - loss: 2.9755e-05 - val_loss: 1.2832e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 92s 20ms/step - loss: 2.6578e-05 - val_loss: 3.1200e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 93s 20ms/step - loss: 2.2002e-05 - val_loss: 1.2047e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 93s 20ms/step - loss: 2.1778e-05 - val_loss: 1.3413e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 93s 20ms/step - loss: 1.9775e-05 - val_loss: 6.0015e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 93s 20ms/step - loss: 1.8679e-05 - val_loss: 1.1879e-05\n",
      "--------------------------\n",
      "\n",
      "Starting time: 1612051070.7877173\n",
      "Completing time: 1612052017.0166736\n",
      "It took -15.770482603708903 minutes to train the model for 1 iterations\n"
     ]
    }
   ],
   "source": [
    "mse_for_iter = []\n",
    "train_loss_over_epoch = []\n",
    "val_loss_over_epoch = []\n",
    "\n",
    "def run_iteration(no_iter=20, epochs = 10):\n",
    "    start_time = time.time()\n",
    "    for iteration in range(no_iter):\n",
    "        print(f'Iteration: {iteration + 1}')\n",
    "        K.clear_session()\n",
    "        model = make_model(X_Train)\n",
    "        #plot_model(model, show_shapes=True)\n",
    "        model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        history = model.fit(X_Train, Y_Train, epochs=epochs,\n",
    "                       validation_data=(X_Test, Y_Test))\n",
    "        train_loss_over_epoch.append(history.history['loss'])\n",
    "        val_loss_over_epoch.append(history.history['val_loss'])\n",
    "        predicted_y_values = model.predict(X_Test)\n",
    "        predicted_y_values_unnor = ytrain_min_max_scaler.inverse_transform(predicted_y_values)\n",
    "        mse_for_iter.append(mean_squared_error(predicted_y_values_unnor, Y_test_actual))\n",
    "               \n",
    "    end_time = time.time()\n",
    "    print('--------------------------\\n')\n",
    "    print(f'Starting time: {start_time}')\n",
    "    print(f'Completing time: {end_time}')\n",
    "    print(f'It took {(start_time - end_time)/60} minutes to train the model for {no_iter} iterations')\n",
    "    #return predicted unnormalized values for test set from last iteration \n",
    "    return predicted_y_values, predicted_y_values_unnor, model\n",
    "\n",
    "predicted_y_values, predicted_y_values_unnor, model = run_iteration(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for X_Train values which has to be used to predict another day output\n",
    "y_train_predict = model.predict(X_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bidur/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a0db6c86f44efdb9963c7e6767d96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=148415.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (148415, 19, 1) (148415,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b2e3ecade04a07bce15f29a78f4492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=37090.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (37090, 19, 1) (37090,)\n"
     ]
    }
   ],
   "source": [
    "#for model2\n",
    "#Building timeseries\n",
    "X_Train1, Y_Train1, Y_train_actual1 = timeseries(x_train_nor, y_train_nor, y_train_actual, time_steps=19, out_steps=1)\n",
    "X_Test1, Y_Test1, Y_test_actual1 = timeseries(x_test_nor, y_test_nor, y_test_actual, time_steps=19, out_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing last value of each timeseries data with the predicted value\n",
    "len_data = len(X_Train1)\n",
    "for i in range (len_data):\n",
    "    X_Train1[i][18] = y_train_predict[i]\n",
    "len_data_test = len(X_Test1)\n",
    "for i in range (len_data_test):\n",
    "    X_Test1[i][18] = predicted_y_values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 136s 29ms/step - loss: 1.2669e-04 - val_loss: 1.0629e-04\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 134s 29ms/step - loss: 6.3306e-05 - val_loss: 4.7114e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 133s 29ms/step - loss: 4.9841e-05 - val_loss: 4.0540e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 134s 29ms/step - loss: 4.1987e-05 - val_loss: 2.9073e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 139s 30ms/step - loss: 3.8054e-05 - val_loss: 3.3387e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 135s 29ms/step - loss: 3.4034e-05 - val_loss: 2.8730e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 136s 29ms/step - loss: 3.1213e-05 - val_loss: 3.3260e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 135s 29ms/step - loss: 2.9386e-05 - val_loss: 1.3147e-04\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 135s 29ms/step - loss: 2.7129e-05 - val_loss: 2.8722e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 135s 29ms/step - loss: 2.4475e-05 - val_loss: 2.0929e-05\n",
      "Iteration: 2\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 139s 30ms/step - loss: 1.2076e-04 - val_loss: 6.9340e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 136s 29ms/step - loss: 5.5206e-05 - val_loss: 3.9805e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 106s 23ms/step - loss: 4.6566e-05 - val_loss: 3.5424e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 3.6118e-05 - val_loss: 2.5892e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 3.7818e-05 - val_loss: 3.7166e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 3.3738e-05 - val_loss: 4.8409e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 2.9410e-05 - val_loss: 2.6439e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 2.6064e-05 - val_loss: 3.1833e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 2.5500e-05 - val_loss: 2.9152e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 2.6121e-05 - val_loss: 2.0087e-05\n",
      "Iteration: 3\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 87s 19ms/step - loss: 1.2403e-04 - val_loss: 9.9444e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 5.8250e-05 - val_loss: 6.2262e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 4.4006e-05 - val_loss: 5.4324e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 4.6168e-05 - val_loss: 6.9433e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 3.8811e-05 - val_loss: 4.3415e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 3.5425e-05 - val_loss: 9.6798e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 2.8920e-05 - val_loss: 4.2942e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 3.0642e-05 - val_loss: 3.4890e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 2.6079e-05 - val_loss: 3.1491e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 2.4085e-05 - val_loss: 1.6970e-05\n",
      "Iteration: 4\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 87s 19ms/step - loss: 1.2545e-04 - val_loss: 5.4907e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 5.9707e-05 - val_loss: 4.8597e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 5.8668e-05 - val_loss: 6.9916e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 4.2449e-05 - val_loss: 3.7347e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 3.2858e-05 - val_loss: 2.4250e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 3.0566e-05 - val_loss: 2.9026e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 3.2152e-05 - val_loss: 2.5477e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 2.7949e-05 - val_loss: 2.1050e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 2.5172e-05 - val_loss: 2.0747e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 2.2765e-05 - val_loss: 2.1131e-05\n",
      "Iteration: 5\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 89s 19ms/step - loss: 1.2160e-04 - val_loss: 4.3627e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 92s 20ms/step - loss: 6.0592e-05 - val_loss: 4.5683e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 5.2086e-05 - val_loss: 4.3549e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 87s 19ms/step - loss: 4.5788e-05 - val_loss: 4.2875e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 87s 19ms/step - loss: 3.9547e-05 - val_loss: 3.3303e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 3.3435e-05 - val_loss: 9.7669e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 3.2472e-05 - val_loss: 3.6042e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 86s 19ms/step - loss: 3.1045e-05 - val_loss: 2.1848e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 111s 24ms/step - loss: 3.0068e-05 - val_loss: 2.4153e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 128s 28ms/step - loss: 2.8797e-05 - val_loss: 4.8330e-05\n",
      "Iteration: 6\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 119s 26ms/step - loss: 1.2230e-04 - val_loss: 4.4624e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 114s 25ms/step - loss: 5.7384e-05 - val_loss: 7.0932e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 119s 26ms/step - loss: 5.1576e-05 - val_loss: 3.4987e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 91s 20ms/step - loss: 4.0885e-05 - val_loss: 2.9826e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 89s 19ms/step - loss: 3.8148e-05 - val_loss: 2.4267e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 89s 19ms/step - loss: 3.4441e-05 - val_loss: 3.7362e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 89s 19ms/step - loss: 3.3929e-05 - val_loss: 2.2984e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 87s 19ms/step - loss: 3.0612e-05 - val_loss: 2.6135e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 2.6591e-05 - val_loss: 1.2702e-04\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 87s 19ms/step - loss: 2.6774e-05 - val_loss: 2.7643e-05\n",
      "Iteration: 7\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 1.2688e-04 - val_loss: 5.1225e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 5.8326e-05 - val_loss: 1.3539e-04\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 4.2906e-05 - val_loss: 3.6731e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 3.9293e-05 - val_loss: 2.5293e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 3.7852e-05 - val_loss: 3.4050e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 3.3019e-05 - val_loss: 3.0362e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 2.8183e-05 - val_loss: 4.9542e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 2.7516e-05 - val_loss: 2.2104e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 2.6129e-05 - val_loss: 1.8004e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 2.4730e-05 - val_loss: 1.5271e-05\n",
      "Iteration: 8\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 89s 19ms/step - loss: 1.3434e-04 - val_loss: 7.8059e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 89s 19ms/step - loss: 5.8869e-05 - val_loss: 5.6260e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 4.7748e-05 - val_loss: 3.5486e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 4.0392e-05 - val_loss: 5.7793e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 81s 18ms/step - loss: 3.5654e-05 - val_loss: 2.5050e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 81s 18ms/step - loss: 3.4612e-05 - val_loss: 4.3667e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 81s 18ms/step - loss: 2.7793e-05 - val_loss: 2.4914e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 3.2141e-05 - val_loss: 5.8705e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 81s 18ms/step - loss: 2.6097e-05 - val_loss: 2.0244e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 2.5832e-05 - val_loss: 1.8033e-05\n",
      "Iteration: 9\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 87s 19ms/step - loss: 1.2427e-04 - val_loss: 7.0235e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 5.7250e-05 - val_loss: 3.8920e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 4.8271e-05 - val_loss: 4.7289e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.9485e-05 - val_loss: 2.8980e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.4748e-05 - val_loss: 3.4939e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 3.2378e-05 - val_loss: 2.3105e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 3.0075e-05 - val_loss: 2.9235e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 2.9026e-05 - val_loss: 3.5396e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 2.6519e-05 - val_loss: 2.2976e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 2.6786e-05 - val_loss: 2.4067e-05\n",
      "Iteration: 10\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 1.2845e-04 - val_loss: 8.5293e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 6.0434e-05 - val_loss: 4.3644e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 4.5662e-05 - val_loss: 3.7206e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 4.1590e-05 - val_loss: 3.2312e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 3.5496e-05 - val_loss: 3.7398e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 3.2574e-05 - val_loss: 2.8942e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 3.1288e-05 - val_loss: 2.2967e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.3414e-05 - val_loss: 4.8573e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 2.6695e-05 - val_loss: 2.0874e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 2.7397e-05 - val_loss: 3.7083e-05\n",
      "Iteration: 11\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 1.1945e-04 - val_loss: 7.1523e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 5.6266e-05 - val_loss: 1.6333e-04\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 4.8062e-05 - val_loss: 4.1134e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 4.2033e-05 - val_loss: 3.2866e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 3.9391e-05 - val_loss: 5.1270e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 3.2578e-05 - val_loss: 2.5718e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 3.6596e-05 - val_loss: 2.8533e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 2.6608e-05 - val_loss: 2.3414e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 2.7522e-05 - val_loss: 2.0807e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 2.4926e-05 - val_loss: 2.7774e-05\n",
      "Iteration: 12\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 1.3972e-04 - val_loss: 5.5453e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 5.9503e-05 - val_loss: 3.8997e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 4.8557e-05 - val_loss: 3.1664e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 4.6215e-05 - val_loss: 3.0537e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.2543e-05 - val_loss: 4.1064e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.4672e-05 - val_loss: 3.2447e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 2.9344e-05 - val_loss: 2.7718e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.0165e-05 - val_loss: 2.3010e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.0589e-05 - val_loss: 2.2363e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 2.6435e-05 - val_loss: 2.5138e-05\n",
      "Iteration: 13\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 1.2756e-04 - val_loss: 1.4999e-04\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 5.7117e-05 - val_loss: 6.2091e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 4.7905e-05 - val_loss: 5.4981e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 93s 20ms/step - loss: 4.1361e-05 - val_loss: 2.8606e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 3.7844e-05 - val_loss: 3.7467e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 3.1915e-05 - val_loss: 2.7804e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 3.0908e-05 - val_loss: 3.4844e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 2.7461e-05 - val_loss: 1.0856e-04\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 2.8602e-05 - val_loss: 1.7293e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 89s 19ms/step - loss: 2.5167e-05 - val_loss: 7.3488e-05\n",
      "Iteration: 14\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 89s 19ms/step - loss: 1.3009e-04 - val_loss: 5.7338e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 89s 19ms/step - loss: 5.9659e-05 - val_loss: 4.3700e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 5.2216e-05 - val_loss: 4.2399e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 4.3236e-05 - val_loss: 7.2314e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 90s 19ms/step - loss: 3.9834e-05 - val_loss: 2.7561e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 3.4224e-05 - val_loss: 4.2170e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 3.5997e-05 - val_loss: 2.8827e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 89s 19ms/step - loss: 3.0575e-05 - val_loss: 2.8438e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 3.0027e-05 - val_loss: 2.1305e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 88s 19ms/step - loss: 2.6392e-05 - val_loss: 6.6833e-05\n",
      "Iteration: 15\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 90s 19ms/step - loss: 1.1376e-04 - val_loss: 6.3606e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 89s 19ms/step - loss: 5.7252e-05 - val_loss: 4.1969e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 90s 19ms/step - loss: 4.8516e-05 - val_loss: 4.4153e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 89s 19ms/step - loss: 4.2345e-05 - val_loss: 2.8506e-05\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638/4638 [==============================] - 82s 18ms/step - loss: 3.5742e-05 - val_loss: 4.2065e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 3.1461e-05 - val_loss: 3.7167e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 3.1221e-05 - val_loss: 4.0814e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 2.7338e-05 - val_loss: 2.4698e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 2.6240e-05 - val_loss: 3.5011e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 2.5828e-05 - val_loss: 2.6814e-05\n",
      "Iteration: 16\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 1.0115e-04 - val_loss: 5.7940e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 5.4891e-05 - val_loss: 4.1715e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 4.9601e-05 - val_loss: 1.8004e-04\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 4.3698e-05 - val_loss: 3.2967e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 3.7607e-05 - val_loss: 3.3182e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 3.0777e-05 - val_loss: 2.6937e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 3.3040e-05 - val_loss: 2.1430e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.0929e-05 - val_loss: 7.7169e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 2.6284e-05 - val_loss: 2.5938e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 2.4018e-05 - val_loss: 2.1894e-05\n",
      "Iteration: 17\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 1.3426e-04 - val_loss: 7.5367e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 6.2830e-05 - val_loss: 6.5707e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 82s 18ms/step - loss: 5.1980e-05 - val_loss: 6.5720e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 4.4050e-05 - val_loss: 6.3100e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 86s 18ms/step - loss: 3.6969e-05 - val_loss: 3.7962e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.7844e-05 - val_loss: 3.0247e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 2.9425e-05 - val_loss: 2.9935e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.2685e-05 - val_loss: 3.0321e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 2.5421e-05 - val_loss: 2.4994e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 2.4878e-05 - val_loss: 2.1280e-05\n",
      "Iteration: 18\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 1.0399e-04 - val_loss: 6.1427e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 5.4716e-05 - val_loss: 4.2660e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 4.4780e-05 - val_loss: 8.3606e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.9078e-05 - val_loss: 2.4309e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.7390e-05 - val_loss: 5.0179e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.3515e-05 - val_loss: 2.9887e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.0130e-05 - val_loss: 2.5630e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 2.9377e-05 - val_loss: 2.5672e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 83s 18ms/step - loss: 3.0939e-05 - val_loss: 2.4777e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 2.2883e-05 - val_loss: 1.7109e-05\n",
      "Iteration: 19\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 1.2932e-04 - val_loss: 9.9115e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 6.3720e-05 - val_loss: 7.2656e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 4.8790e-05 - val_loss: 3.3132e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 4.0188e-05 - val_loss: 4.2947e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 3.5883e-05 - val_loss: 3.1852e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 3.8325e-05 - val_loss: 9.4871e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 3.2607e-05 - val_loss: 2.8504e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 2.9425e-05 - val_loss: 2.6051e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 2.8564e-05 - val_loss: 2.0598e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 92s 20ms/step - loss: 2.6244e-05 - val_loss: 4.4536e-05\n",
      "Iteration: 20\n",
      "Epoch 1/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 1.2069e-04 - val_loss: 5.8086e-05\n",
      "Epoch 2/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 6.0313e-05 - val_loss: 3.6687e-05\n",
      "Epoch 3/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 4.5390e-05 - val_loss: 3.2587e-05\n",
      "Epoch 4/10\n",
      "4638/4638 [==============================] - 85s 18ms/step - loss: 3.7467e-05 - val_loss: 2.9780e-05\n",
      "Epoch 5/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 3.8819e-05 - val_loss: 2.4721e-05\n",
      "Epoch 6/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 3.3071e-05 - val_loss: 2.4285e-05\n",
      "Epoch 7/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 2.8816e-05 - val_loss: 4.4719e-05\n",
      "Epoch 8/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 3.0216e-05 - val_loss: 2.7118e-05\n",
      "Epoch 9/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 2.7331e-05 - val_loss: 1.9330e-05\n",
      "Epoch 10/10\n",
      "4638/4638 [==============================] - 84s 18ms/step - loss: 2.4942e-05 - val_loss: 2.7789e-05\n",
      "--------------------------\n",
      "\n",
      "Starting time: 1612052047.967659\n",
      "Completing time: 1612070079.3828335\n",
      "It took -300.5235862414042 minutes to train the model for 20 iterations\n"
     ]
    }
   ],
   "source": [
    "mse_for_iter1 = []\n",
    "train_loss_over_epoch1 = []\n",
    "val_loss_over_epoch1 = []\n",
    "predicted_y_values1_unnors = []\n",
    "models = []\n",
    "\n",
    "def run_iteration_model_2(no_iter=20, epochs = 10):\n",
    "    start_time = time.time()\n",
    "    for iteration in range(no_iter):\n",
    "        print(f'Iteration: {iteration + 1}')\n",
    "        K.clear_session()\n",
    "        model1 = make_model(X_Train1)\n",
    "        #plot_model(model, show_shapes=True)\n",
    "        model1.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        history = model1.fit(X_Train1, Y_Train1, epochs=epochs,\n",
    "                       validation_data=(X_Test1, Y_Test1))\n",
    "        train_loss_over_epoch1.append(history.history['loss'])\n",
    "        val_loss_over_epoch1.append(history.history['val_loss'])\n",
    "        predicted_y_values1 = model1.predict(X_Test1)\n",
    "        predicted_y_values1_unnor = ytrain_min_max_scaler.inverse_transform(predicted_y_values1)\n",
    "        mse_for_iter1.append(mean_squared_error(predicted_y_values1_unnor, Y_test_actual1))\n",
    "        predicted_y_values1_unnors.append(predicted_y_values1_unnor)\n",
    "        models.append(model1)\n",
    "               \n",
    "    end_time = time.time()\n",
    "    print('--------------------------\\n')\n",
    "    print(f'Starting time: {start_time}')\n",
    "    print(f'Completing time: {end_time}')\n",
    "    print(f'It took {(start_time - end_time)/60} minutes to train the model for {no_iter} iterations')\n",
    "    #return predicted unnormalized values for test set from last iteration \n",
    "    return predicted_y_values1, predicted_y_values1_unnor, model1\n",
    "\n",
    "predicted_y_values1, predicted_y_values1_unnor, model1 = run_iteration_model_2(20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lstm_model2_single_features.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation():\n",
    "    #mean square error over iterations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(mse_for_iter1)\n",
    "    plt.ylabel('mean squared error')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.savefig('mse.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "        \n",
    "    #plot train and validation loss over epochs for each iteration\n",
    "    fig, axs = plt.subplots(10, 2)\n",
    "    #axs.set_ylabel('loss')\n",
    "    #axs.set_xlabel('epoch')\n",
    "    fig.set_size_inches(18.5, 50)\n",
    "    for row in range(10):\n",
    "        for col in range(2):\n",
    "            index = 2*row + col #index to get loss from training loss and validation loss list\n",
    "            axs[row, col].plot(train_loss_over_epoch1[index])\n",
    "            axs[row, col].plot(val_loss_over_epoch1[index])\n",
    "            axs[row, col].set_title('Iteration(%d)' %(index+1))\n",
    "            axs[row, col].legend(['train', 'Val'], loc='upper left')            \n",
    "    plt.savefig('train_val_loss.png', bbox_inches='tight')\n",
    "    \n",
    "    #plot predicted value vs actual values from last iteration\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(predicted_y_values1_unnor, 'ro', alpha=0.8)\n",
    "    plt.plot(Y_test_actual1, 'bo', alpha=0.1)\n",
    "    plt.legend(['Predicted values', 'Actual values'], loc='upper right')\n",
    "    plt.ylabel('Outflow')\n",
    "    plt.xlabel('nth datapoint')\n",
    "    plt.savefig('predicted_values_visuals.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "#call plot_evaluation function\n",
    "plot_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_for_iter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mse_for_iter1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model1, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
